{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import product\n",
    "import timeit\n",
    "import pickle\n",
    "from os import path, getenv\n",
    "import difflib\n",
    "from statistics import mean\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(context, command):\n",
    "    framing = f'You are an AI that controls a smart home.'\n",
    "    context = f'Here is the state of the devices in the home, in JSON format: {json.dumps(context)}'\n",
    "    command = f'The user issues the command: {command}. Change the device state as appropriate.'\n",
    "    formatting = f'Provide your response in JSON format.'\n",
    "\n",
    "    return f'{framing} {context} {command} {formatting}'\n",
    "\n",
    "def query_chatgpt(prompt):\n",
    "    response = openai.Completion.create(\n",
    "      model='text-davinci-003',\n",
    "      prompt=prompt,\n",
    "      temperature=0.7,\n",
    "      max_tokens=2000,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test contexts\n",
    "with open('./contexts.json', 'r') as f:\n",
    "    contexts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test commands\n",
    "with open('./commands.json', 'r') as f:\n",
    "    commands = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = list(product(contexts.keys(), commands.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(trial):\n",
    "    context = contexts[trial[0]]\n",
    "    command = commands[trial[1]]\n",
    "    prompt = make_prompt(context, command)\n",
    "\n",
    "    t0 = timeit.default_timer()\n",
    "    response = query_chatgpt(prompt)\n",
    "    t1 = timeit.default_timer()\n",
    "\n",
    "    result = {\n",
    "        'context': context,\n",
    "        'command': command,\n",
    "        'prompt': prompt,\n",
    "        'response': response,\n",
    "        'time_elapsed': t1 - t0\n",
    "    }\n",
    "\n",
    "    with open(f'./results/pickles/v1/{trial[0]}-context_{trial[1]}-command.pkl', 'ab') as f:\n",
    "        pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "for trial in trials:\n",
    "    for _ in range(0, N):\n",
    "        run_trial(trial)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Results\n",
    "This translates the responses into human-readable Markdown files so they can be easily assessed for quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(trial):\n",
    "    results = []\n",
    "\n",
    "    with open(f'./results/pickles/v1/{trial[0]}-context_{trial[1]}-command.pkl', 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                results.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                break\n",
    "\n",
    "    return results\n",
    "\n",
    "def markdownify_results(trial):\n",
    "    results = load_results(trial)\n",
    "\n",
    "    with open(f'./results/markdown/v1/{trial[0]}-context_{trial[1]}-command.md', 'w') as f:\n",
    "        f.write(f'# {trial[0]} context, {trial[1]} command\\n\\n')\n",
    "        f.write(f'## Instructions\\n\\n')\n",
    "        f.write(f'For every trial in this file, please assign a binary score (0 or 1) based on the following:\\n\\n')\n",
    "        f.write(f'- Poor response (0): The changes to the devices do not at all reflect the intent behind the user command, or the response is malformed/garbled.\\n')\n",
    "        f.write(f'- Good response (1): The changes to the devices are reasonable for the command. You can imagine _someone_ \\\n",
    "            being satisfied with the result, even if it is somewhat subjective (e.g., based on different personal preferences).\\n\\n')\n",
    "\n",
    "        for i, ln in enumerate(results):\n",
    "            f.write(f'## Trial {i} - {trial[0]} context, {trial[1]} command \\n\\n')\n",
    "            f.write(f'command: **{ln[\"command\"]}**\\n\\n')\n",
    "            \n",
    "            context = json.dumps(ln['context'], indent=2)\n",
    "            response = json.loads(ln['response']['choices'][0]['text'])\n",
    "            response = json.dumps(response, indent=2)\n",
    "            \n",
    "            diff = difflib.ndiff(context.splitlines(keepends=True), response.splitlines(keepends=True))\n",
    "            f.write(f'chatgpt\\'s changes to the devices: \\n\\n``` json\\n{\"\".join(diff)}\\n```\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate the responses into neat markdown for qualitative analysis\n",
    "for trial in trials:\n",
    "    markdownify_results(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('simple', 'simple'): 2.91 average latency\n",
      "('simple', 'medium'): 3.71 average latency\n",
      "('simple', 'ambiguous'): 3.86 average latency\n",
      "('medium', 'simple'): 6.64 average latency\n",
      "('medium', 'medium'): 6.25 average latency\n",
      "('medium', 'ambiguous'): 6.43 average latency\n",
      "('complex', 'simple'): 9.71 average latency\n",
      "('complex', 'medium'): 9.16 average latency\n",
      "('complex', 'ambiguous'): 11.73 average latency\n"
     ]
    }
   ],
   "source": [
    "# compute the average latency for all experiments\n",
    "for trial in trials:\n",
    "    results = load_results(trial)\n",
    "    latencies = [r['time_elapsed'] for r in results]\n",
    "    print(f'{trial}: {mean(latencies):.2f} average latency')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0de3b845acd206eb77ce2e1febe382fd31487301bf5ada74fe5931c27840861b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
